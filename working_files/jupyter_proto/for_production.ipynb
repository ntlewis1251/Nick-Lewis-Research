{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542a5e74",
   "metadata": {},
   "source": [
    "# Knickpoint Analyses for Geoscience\n",
    "-Nick Lewis, Joanmarie Del Vecchio, Chuck Bailey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e099c",
   "metadata": {},
   "source": [
    "Steps\n",
    "- Get Dem (take user bounds, res.)\n",
    "- Download geology shp files\n",
    "- Download WBD GDB\n",
    "- User can now select what they want to do.\n",
    "Export files include:\n",
    "\t- Folders for each watershed w name\n",
    "        - Relief map\n",
    "            - W/ LSHT highlighted\n",
    "\t    - Slope map (similar but not the same!)\n",
    "            - LSHT\n",
    "\t    - Knickpoints mapped (maybe adjust delta ksn?  To isolate bedrock reaches.)\n",
    "\t\t- Profile w trimodal morphology represented (maybe, refer to chuck suggestion)\n",
    "\t- CSVs of knickpoints\n",
    "\t\t- To be included:\n",
    "        - Wshed\n",
    "\t\t- elevation\n",
    "\t\t- Coords\n",
    "\t\t- Shapefile geology\n",
    "\t\t- Nearest roadcut geology that is also within some threshold of elevation from knickpoint.  Do buffer around knickpoint select closest to elevation.\n",
    "\t\t- Stream cover type (to allow narrowing to bedrock)\n",
    "\t\t- If BR, fracture data, if AP, bedding\n",
    "\t\t- If not in AP or BR, Send back a message being like sorry dawg.\n",
    "\t\t- Depth to mantle @ knickpoint.  Z score to average depth in region.\n",
    "\t\t- Slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cd715",
   "metadata": {},
   "source": [
    "# Code begins here!\n",
    "\n",
    "#### Below is a 'code cell'.  This is a block of text that Jupyter and Python use to perform tasks.\n",
    "\n",
    "#### To run the following code cell and all code cells below, select the cell and press\n",
    "# <p style=\"color: aqua;\">Shift+Enter.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9159ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is called a comment.  Denoted by the '#' behind the text, it allows me to better explain code cells while still being a begign line of text.\n",
    "\n",
    "# First, we import the YML file.  This saves you the work of making your own python environment and installing a bunch of packages.\n",
    "#!conda env create -f env.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2eb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From google drive, let's get the peripheral files our code will use.\n",
    "#function here that imports shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbb3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages to our notebook.\n",
    "# We must do this to use the functions contained inside the packages.\n",
    "# Some functions have 'as' in the import call.  This is for less typing, and nothing else.\n",
    "# Google any package name and 'docs' in order to find more detailed explanation of any one package.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geopandas as gpd\n",
    "import xdem\n",
    "import os\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import shapely\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a698607",
   "metadata": {},
   "source": [
    "#### Phew!  That was a lot of initial stuff, but no worries.  We can start *really* coding now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e038a",
   "metadata": {},
   "source": [
    "#### Below is a code cell containing our **Constant** Variables, which you may change as you see fit for your particular analyses.\n",
    "\n",
    "#### Below are \n",
    "- Integers\n",
    "- Lists (denoted by square brackets at the beginning and end, separated by commas [a, b, c, d])\n",
    "- Strings (contained within quotes 'Like this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d73e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time you change something within the code cell, run it once more to update the variable.\n",
    "\n",
    "BOUNDS = [39.0, -80.0, 37.5, -81.5] # In decimal coordinate form.  Format: [N, E, S, W].  Select the smallest area possible in your analyses for maximum resoultion!  \n",
    "# For now, keep them in Virginia or West VA.\n",
    "\n",
    "API_KEY = '9086887f175d33aa72eda767f5b1e9cd' # Get your API key from opentopography.org.  Students get higher resolution data!\n",
    "\n",
    "DEM_DATASET = 'USGS30m' # Options are USGS30m, USGS10m, & USGS1m.  Be warned!  As resolution goes up, available area goes down.\n",
    "\n",
    "PROJ_TITLE = 'n_lewis' # Create a succinct name with no spaces or leading digits to represent your project file for future exports.\n",
    "\n",
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb2a604",
   "metadata": {},
   "source": [
    "#### If you can't remember what a function does, call the help() function and pass the name of the function you are wondering about inside the colons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4d1159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory \"/sciclone/home/ntlewis/Nick-Lewis-Research/working_files/jupyter_proto/n_lewis\" already exists.\n",
      "Querying API...\n",
      "URL passed, now waiting for file to write.\n",
      "File Written.\n"
     ]
    }
   ],
   "source": [
    "# Downloader\n",
    "def downloader(bounds:list = BOUNDS, api_key:str = API_KEY, dem_dataset:str = DEM_DATASET, proj_title:str = PROJ_TITLE, path = PATH): # To call a function, we have to define it first.\n",
    "    \"\"\"\n",
    "    This function takes in the user-defined constants and creates a directory.  It then places a geotiff in the directory.\n",
    "\n",
    "    args: Bounds (list), api_key (str), dem_dataset (str), proj_title (str).\n",
    "\n",
    "    returns: None.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, we make our directory for project files.\n",
    "    try:\n",
    "        os.mkdir(path = f'{path}/{proj_title}')\n",
    "        print(f'Directory \"{path}/{proj_title}\" created successfully.') # Tries to make the directory for project files.\n",
    "        proj_dir = f'{path}/{proj_title}'\n",
    "    except FileExistsError:\n",
    "        print(f'Directory \"{path}/{proj_title}\" already exists.') # If it doesnt work, it'll tell you!\n",
    "        proj_dir = f'{path}/{proj_title}'\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {e}')\n",
    "\n",
    "    # Now to download our dem.\n",
    "    dic = dict(zip(['north', 'east', 'south', 'west'], bounds)) # Makes a dictionary to let us more easily acess our bounds.\n",
    "    url = f'https://portal.opentopography.org/API/usgsdem?datasetName={dem_dataset}&south={dic[\"south\"]-0.5}&north={dic[\"north\"]+0.5}&west={dic[\"west\"]-0.5}\\\n",
    "&east={dic[\"east\"]+0.5}&outputFormat=GTiff&API_Key={api_key}' # Creates our URL so we can download.\n",
    "    print('Querying API...')\n",
    "    response = requests.get(url) # Saves what the URL spit back at us\n",
    "    print('URL passed, now waiting for file to write.')\n",
    "    with open(f'{proj_dir}/gtiff.tiff', 'wb') as file:\n",
    "        file.write(response.content) # Writes the response to our project file\n",
    "    print('File Written.')\n",
    "\n",
    "downloader() # Here, we call the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7749e",
   "metadata": {},
   "source": [
    "#### Next we will clip out each watershed in your selected area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc830b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Directory \"/sciclone/home/ntlewis/Nick-Lewis-Research/working_files/jupyter_proto/n_lewis/wshed_dems\" already exists.\n",
      "Writing new DEMs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 67/67 [00:06<00:00, 10.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Wshed Clipper\n",
    "def wshed_clipper(proj_title = PROJ_TITLE, bounds = BOUNDS, path = PATH):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # PLACEHOLDER - Line to download wsheds csv\n",
    "    # df = pd.read_csv(f'{os.getcwd}/{proj_title}/wsheds.csv') # We read in our wshed data\n",
    "    print('Loading data...')\n",
    "    df = pd.read_csv('/sciclone/home/ntlewis/Nick-Lewis-Research/working_files/data/wbd_scale/WBDHU10.csv') \n",
    "    gdf = gpd.GeoDataFrame(data=df, crs=rio.CRS.from_epsg(4269), geometry=[shapely.wkt.loads(x) for x in df['geometry']]) # We turn it into a geodataframe to do geospatial stuff to.\n",
    "    clipped = gpd.GeoDataFrame(gdf.cx[bounds[3]:bounds[1], bounds[2]:bounds[0]]) # Clip it to the user's bounds.\n",
    "\n",
    "    # Now that we have a GDF (GeoDataFrame) with the user's chosen watersheds and the larger dem, lets export DEMs of each watershed.\n",
    "    try:\n",
    "        os.mkdir(path = f'{path}/{proj_title}/wshed_dems') # Makes a directory to store these new dems.\n",
    "        print(f'Directory \"{path}/{proj_title}/wshed_dems\" created successfully.') # Tries to make the directory for project files.\n",
    "    except FileExistsError:\n",
    "        print(f'Directory \"{path}/{proj_title}/wshed_dems\" already exists.') # If it doesnt work, it'll tell you!\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {e}')\n",
    "    \n",
    "    # We have our directory, lets populate it.\n",
    "    print('Writing new DEMs...')\n",
    "    with rio.open(f'{path}/{proj_title}/gtiff.tiff') as src: # Loads in our larger df.\n",
    "\n",
    "        total_tasks=len(clipped)\n",
    "        with tqdm(total=total_tasks, desc='Progress') as pbar: # This is for the fancy progress bar.\n",
    "\n",
    "            for idx, row in clipped.iterrows(): # Iterate through the watershed dataframe (for each row, do something.)\n",
    "                name = row['name'].replace(' ', '_') # Replaces spaces with underscores and saves the name to a variable.\n",
    "\n",
    "                geom = row['geometry'] # Saves our geometry to a variable.\n",
    "\n",
    "                out_image, out_transform = rio.mask.mask(src, geom.geoms, crop=True, nodata=-99999) # Clips the DEM to our watershed's shape.  Sets nodata value to LSDTopyTools standard.\n",
    "\n",
    "                # Below is metadata stuff, dont stress about this.\n",
    "                out_meta = src.meta.copy()\n",
    "                out_meta.update({\n",
    "                    \"driver\": \"GTiff\",\n",
    "                    \"height\": out_image.shape[1],\n",
    "                    \"width\": out_image.shape[2],\n",
    "                    \"transform\": out_transform\n",
    "                    })\n",
    "\n",
    "                out_path = f'{path}/{proj_title}/wshed_dems/{name}.tiff' # Create a pathname.\n",
    "                with rio.open(out_path, \"w\", **out_meta) as file:\n",
    "                    file.write(out_image) # Write to file.\n",
    "                pbar.update(1)\n",
    "\n",
    "# Call our function, let it do the work.           \n",
    "wshed_clipper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in glob.glob('/sciclone/home/ntlewis/Nick-Lewis-Research/working_files/jupyter_proto/n_lewis/wshed_dems/*.tiff'):\n",
    "    with rio.open(x) as src:\n",
    "        show(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598495af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relief & LSHT toolbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KSN toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf67aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b71a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing tasks: 100%|██████████| 100/100 [00:05<00:00, 19.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_tasks = 100\n",
    "with tqdm(total=total_tasks, desc=\"Performing tasks\") as pbar:\n",
    "    for i in range(total_tasks):\n",
    "        # Simulate some work\n",
    "        time.sleep(0.05)\n",
    "        pbar.update(1) # Increment the progress bar by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kept to show how wshed dataset was narrowed.  U10 scale and only regions in the appalachians.\n",
    "#apps = gdf.cx[-87:-58, 32:49]\n",
    "#apps.to_csv('/sciclone/home/ntlewis/Nick-Lewis-Research/working_files/data/wbd_scale/WBDHU10.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
